{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gjknJHb41P"
      },
      "source": [
        "# LLM Proxy User Documentation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/derickson/ExpressOpenAIChatProxy/blob/main/Notebooks/LLM_Proxy.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SraMUuuAizVr"
      },
      "outputs": [],
      "source": [
        "! pip install -q openai\n",
        "! pip install -q langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEodECx8pyZX"
      },
      "source": [
        "## How to use the proxy with OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "my_proxy_key = getpass(\"Enter the instructor provided proxy key: \")\n",
        "my_short_id = input(\"Enter a short user id to uniquely identify you: \")\n",
        "my_combo_key = f\"{my_proxy_key}-{my_short_id}\""
      ],
      "metadata": {
        "id": "TvIQt2Khb8iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfTVM9Cgi1EK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Load your API key from an environment variable or secret management service\n",
        "openai.api_key = my_combo_key\n",
        "openai.api_base = \"https://llmproxy.gcp.elasticsa.co/v1\"\n",
        "\n",
        "chat_completion = None\n",
        "try:\n",
        "  chat_completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": \"Hello world\"}\n",
        "          ]\n",
        "      )\n",
        "  print(chat_completion)\n",
        "except openai.error.OpenAIError as e:\n",
        "    # Handle exceptions here, if any\n",
        "    print(\"Error occurred:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ikUeY4p7TH"
      },
      "source": [
        "## How to use the proxy with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCTRqooQjjCP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "openai.api_key = my_combo_key\n",
        "openai.api_base = \"https://llmproxy.gcp.elasticsa.co/v1\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
        "\n",
        "openai_chat = ChatOpenAI()\n",
        "\n",
        "PROMPT = \"Hello!\"\n",
        "\n",
        "response = openai_chat([HumanMessage(content=PROMPT)]).content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big Chat Loop with LangChain\n",
        "\n",
        "this is the last step of the lab part 1. Let's chat with an AI bot."
      ],
      "metadata": {
        "id": "j_vsdVN5_bPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import ConversationChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "import json\n",
        "# pretty printing JSON objects\n",
        "def json_pretty(input_object):\n",
        "  print(json.dumps(input_object, indent=4))\n",
        "\n",
        "\n",
        "import textwrap\n",
        "# wrap text when printing, because colab scrolls output to the right too much\n",
        "def wrap_text(text, width):\n",
        "    wrapped_text = textwrap.wrap(text, width)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n",
        "template = \"\"\"The following is a serious conversation between a human and a TV\n",
        "News Anchor named Newsy McNewserson.\n",
        "The Anchor provides autoritative information and commentary in short responses.\n",
        "If the Anchor does not know the answer to a question,\n",
        "he truthfully says it does not know.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "Anchor:\"\"\"\n",
        "\n",
        "MEMORY = ConversationBufferWindowMemory(ai_prefix=\"Anchor\", k=2)\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=openai_chat,\n",
        "    verbose=False,\n",
        "    memory=MEMORY,\n",
        ")\n",
        "\n",
        "def chatLoop():\n",
        "  print(\" -- Have a conversation with a TV news Anchor: \")\n",
        "  print(\" -- Ask this AI \\\"what is in the news?\\\" \")\n",
        "  print(\" -- type 'exit' when done\")\n",
        "\n",
        "  user_input = input(\"> \")\n",
        "  while not user_input.lower().startswith(\"exit\"):\n",
        "      print( wrap_text(conversation.run(user_input),70) )\n",
        "      print(\" -- type 'exit' when done\")\n",
        "      user_input = input(\"> \")\n",
        "  print(\"\\n -- end conversation --\")"
      ],
      "metadata": {
        "id": "YPDDiQ3A_alA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## start a new chat each time\n",
        "MEMORY.clear()\n",
        "## start the chat\n",
        "chatLoop()"
      ],
      "metadata": {
        "id": "vZdZe3ju__RU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}