{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gjknJHb41P"
      },
      "source": [
        "# LLM Proxy User Documentation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/derickson/ExpressOpenAIChatProxy/blob/main/Notebooks/LLM_Proxy.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SraMUuuAizVr"
      },
      "outputs": [],
      "source": [
        "! pip install -q openai\n",
        "! pip install -q langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEodECx8pyZX"
      },
      "source": [
        "## How to use the proxy with OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "my_proxy_key = getpass(\"Enter the instructor provided proxy key: \")\n",
        "my_short_id = input(\"Enter a short user id to uniquely identify you: \")\n",
        "my_combo_key = f\"{my_proxy_key}-{my_short_id}\""
      ],
      "metadata": {
        "id": "TvIQt2Khb8iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfTVM9Cgi1EK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Load your API key from an environment variable or secret management service\n",
        "openai.api_key = my_combo_key\n",
        "openai.api_base = \"https://llmproxy.gcp.elasticsa.co/v1\"\n",
        "\n",
        "chat_completion = None\n",
        "try:\n",
        "  chat_completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": \"Hello world\"}\n",
        "          ]\n",
        "      )\n",
        "  print(chat_completion)\n",
        "except openai.error.OpenAIError as e:\n",
        "    # Handle exceptions here, if any\n",
        "    print(\"Error occurred:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ikUeY4p7TH"
      },
      "source": [
        "## How to use the proxy with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCTRqooQjjCP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "openai.api_key = my_combo_key\n",
        "openai.api_base = \"https://llmproxy.gcp.elasticsa.co/v1\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
        "\n",
        "openai_chat = ChatOpenAI()\n",
        "\n",
        "PROMPT = \"Hello!\"\n",
        "\n",
        "response = openai_chat([HumanMessage(content=PROMPT)]).content\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}